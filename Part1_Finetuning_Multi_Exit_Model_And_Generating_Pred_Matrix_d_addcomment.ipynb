{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZrkmpkW8Wbj"
   },
   "source": [
    "# Part-1 : Section A\n",
    "Training a multi-exit ElasticBERT model on SST-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "## 에러분석을 위해 추가한 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "\n",
    "## issue 152번을 참고하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CkfzdLG9C1T",
    "outputId": "a7400ed9-42af-4197-ba5e-a1b1bb03fa34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/divya/UBERT/MutiExitDNNs/ElasticBERT'\n",
      "/home/aix7101/jeong/CeeBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "# The code closely follows the original ElasticBERT repository\n",
    "# Feature to train models with a given exit configuration is added\n",
    "# !git clone https://github.com/MLiONS/MutiExitDNNs.git\n",
    "\n",
    "\n",
    "%cd /home/divya/UBERT/MutiExitDNNs/ElasticBERT\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 현재 위치 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd\n",
    "\n",
    "# 현재 위치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A26ZgAoxPyiJ",
    "outputId": "93510bc8-6217-4989-c9e9-24beef747003"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_output_layers를 exit-configuration에 맞게 바꾸기'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All the hyper-parameters/ location to training dataset are set in\n",
    "#MultiExitDNNs -> finetune-dynamic -> finetune_elue_entropy.sh file\n",
    "\"\"\"모든 hyper-parameter는 multiExitDNNs/finetune-dynamic/finetune_elue_entropy.sh에 기재되어있음\"\"\"\n",
    "\n",
    "#1)Set the correct location to SST-2 dataset\n",
    "\"\"\"SST-2 데이터셋을 올바른 곳에 위치시키기\"\"\"\n",
    "#All models are trained on SST-2 \"train\" split and evaluated on \"dev\" split\n",
    "#\"train.tsv\" and \"dev.tsv\" are expected to be in ELUE_DIR/TASK_NAME\n",
    "#You can set both ELUE_DIR and TASK_NAME in finetune_elue_entropy.sh\n",
    "#Or change the dataset directory using \"data_dir\" option\n",
    "\"\"\"모든 모델은 SST-2의 train에 대해 학습된다. (train은 train/dev로 split)\n",
    "위치는 ELUE_DIR/TASK_NAME에 저장되고, ELUE_DIR, TASK_NAME 모두 finetune_elue_entropy.sh에서 수정 가능\n",
    "데이터 위치도 data_dir를 통해 수정할 수 있음\"\"\"\n",
    "\n",
    "#2)Please change the \"num_output_layers\" option as per the desired exit-configuration\n",
    "\"num_output_layers를 exit-configuration에 맞게 바꾸기\"\n",
    "\n",
    "#3)Model checkpoints will be saved at \"output_dir\" and\n",
    "#logs will be available at \"log_dir\"\n",
    "# bash finetune_elue_entropy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tfcWxU28kkl"
   },
   "source": [
    "# Part-1 : Section B\n",
    "Generating the prediction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "acoXLlG2y1ty"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIMDb: 영화 감상에 대한 분류 (binary(+,-))\\nYelp: 식당에 대한 리뷰 분류 (binary or 5 classes -> 여기선 binary)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation on other datasets-IMDb or Yelp\n",
    "# IMDB나 Yelp를 이용해서 Evaluation\n",
    "\"\"\"\n",
    "IMDb: 영화 감상에 대한 분류 (binary(+,-))\n",
    "Yelp: 식당에 대한 리뷰 분류 (binary or 5 classes -> 여기선 binary)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aix7101/jeong/CeeBERT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aix7101/jeong/CeeBERT/ElasticBERT/finetune-dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ElasticBERT/finetune-dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kb9mBUI5y1q-",
    "outputId": "f06bf2bb-9a73-407f-b80d-8b5064d1cc1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer as ElasticBertTokenizer\n",
    "\n",
    "#Set the current directory location inside \"finetune-dynamic\" folder\n",
    "\"\"\"현재 directory가 finetune-dynamic이어야 아래 mudle 사용 가능\"\"\"\n",
    "# cd /home/divya/UBERT/MutiExitDNNs/ElasticBERT/finetune-dynamic\n",
    "\n",
    "from models.configuration_elasticbert import ElasticBertConfig\n",
    "from models.modeling_elasticbert_entropy import ElasticBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/aix7101/jeong/fnlp/elasticbert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZkT2IrL2b90d"
   },
   "outputs": [],
   "source": [
    "#Set location to the best performing model\n",
    "#Model checkpoints are saved at \"output_dir\" from Part-1: Section A\n",
    "checkpoint_snli = path + '/ckpts/elue/entropy/SNLI/checkpoint-25700'\n",
    "checkpoint_sst = path + '/ckpts/elue/entropy/SST-2/checkpoint-300'\n",
    "checkpoint_mrpc = path + '/ckpts/elue/entropy/MRPC/checkpoint-575'\n",
    "checkpoint_scitail = path + '/ckpts/elue/entropy/SciTail/checkpoint-3690'\n",
    "checkpoint_rte = path + '/ckpts/elue/entropy/RTE/checkpoint-390'\n",
    "checkpoint_mnli = path + '/ckpts/elue/entropy/MNLI/checkpoint-61360'\n",
    "checkpoint_qnli = path + '/ckpts/elue/entropy/QNLI/checkpoint-16370'\n",
    "checkpoint_qqp = path + '/ckpts/elue/entropy/QQP/checkpoint-56855'\n",
    "\n",
    "# 모델 불러오기. 이 때 학습 데이터를 sst로 했으므로 sst만 가져옴. checkpoint는 왜 삼백으로 설정했는진 모르겠음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cCv-3N0CaM0s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/transformers/modeling_utils.py:1205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "config = ElasticBertConfig.from_pretrained(checkpoint_sst)\n",
    "tokenizer = ElasticBertTokenizer.from_pretrained(checkpoint_sst)\n",
    "model = ElasticBertForSequenceClassification.from_pretrained(checkpoint_sst)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AFt7llc_g130"
   },
   "outputs": [],
   "source": [
    "def get_args(arg_vec):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--num_hidden_layers\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='The number of layers to import.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_output_layers\",\n",
    "        nargs = 12,\n",
    "        default=None,\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='The number of layers to output.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to pre-trained model or shortcut name.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--task_name\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The name of the task to train selected in the list.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the logs will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--spec_eval\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"'Set as train or test based on specific split on which to evaluate'\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\",\n",
    "        default='0',\n",
    "        type=str,\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--regression_threshold\",\n",
    "        default=0,\n",
    "        type=float,\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--early_exit_entropy\",\n",
    "        default='0.1',\n",
    "        type=str,\n",
    "        required=False,\n",
    "    )\n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--load\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"The path of ckpts used to continue training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained config name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "             \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Whether to use debug mode.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\n",
    "        \"--evaluate_during_training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Run evaluation during training at each logging step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Set this flag if you are using an uncased model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_train_batch_size\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU/CPU for training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU/CPU for evaluation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        default=5e-5,\n",
    "        type=float,\n",
    "        help=\"The initial learning rate for Adam.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        default=3.0,\n",
    "        type=float,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--warmup_rate\", default=0, type=float, help=\"Linear warmup over warmup_rate.\")\n",
    "\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Save checkpoint every X updates steps.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_output_dir\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite the content of the output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite the cached training and evaluation sets\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--not_save_model\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Do not save model checkpoints\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=6, help=\"random seed for initialization\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local_rank\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"For distributed training: local_rank\",\n",
    "    )\n",
    "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    args = parser.parse_args(arg_vec)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.config.num_labels = 2\n"
     ]
    }
   ],
   "source": [
    "print(\"model.config.num_labels =\", model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- loss 계산을 위해 num_labels를 task에 맞춰서 변경해줘야함\n",
    "## 근데 그건 task 따라서 다름 내가 하려는 건 이진분류니까 바꿔줄 필요 딱히 없음\n",
    "\n",
    "# model.config.num_labels = 3\n",
    "# model.classifier = torch.nn.Linear(model.config.hidden_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"model.config.num_labels =\", model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E2cX_ym5pJwl"
   },
   "outputs": [],
   "source": [
    "from load_data import (\n",
    "    load_and_cache_examples_glue,\n",
    "    load_and_cache_examples_elue,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_elue_entropy(args, model, tokenizer, prefix=\"\", eval_highway=False, entropy=0.): # eval_highway: 실제 early exit까지 활성화하여 평가\n",
    "    model.elasticbert.set_early_exit_entropy(entropy) # early stop을 위한 entropy지정, entropy를 기준으로 early exit 활성화\n",
    "    model.elasticbert.set_eval_state(eval_highway) # evaluation할 때 True이면 이 때도 early exit를 사용함\n",
    "    model.elasticbert.reset_stats() # early exit 과정에서 생긴 내부 통계들을 리셋, 새로운 실험 시작 전에 사용\n",
    "\n",
    "    eval_task = args.task_name.lower()\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    num_op_layers = args.num_output_layers # early exit가 있는 수, 최대 12개, 12개의 값을 받아들임 (1,0으로 1이면 early exit 존재)\n",
    "\n",
    "    results = {}\n",
    "    results_all = []\n",
    "    exit_layer = []\n",
    "    for i in range(sum(num_op_layers)):\n",
    "        results_all.append({}) # early exit 수만큼 dictionary를 results_all에 append\n",
    "\n",
    "    ##- 데이터셋 로딩\n",
    "    if args.spec_eval: # default = None / 해당 경우 train을 사용\n",
    "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type=args.spec_eval)\n",
    "    else:\n",
    "      eval_dataset = load_and_cache_examples_elue(args, eval_task, tokenizer, data_type='train')\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu eval\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    preds_all = []\n",
    "    pred_tuple = []\n",
    "    for i in range(sum(num_op_layers)):\n",
    "        preds_all.append(None)\n",
    "        pred_tuple.append(None)\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        labels = batch[-1]\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[-1],\n",
    "            }\n",
    "            inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs) # outputs = (loss, logits, hidden_states, attentions)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if out_label_ids is None:\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                if pred is None:\n",
    "                    preds_all[i] = logits[i].detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds_all[i] = np.append(pred, logits[i].detach().cpu().numpy(), axis=0)\n",
    "        else:\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args.output_mode == \"classification\":\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                preds_all[i] = np.argmax(pred, axis = 1)\n",
    "                pred_tuple[i] = pred\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis = 1)\n",
    "            pred_tuple[i] = pred\n",
    "\n",
    "    elif args.output_mode == \"regression\":\n",
    "        if not eval_highway:\n",
    "            for i, pred in enumerate(preds_all):\n",
    "                preds_all[i] = np.squeeze(pred)\n",
    "        else:\n",
    "            preds = np.squeeze(preds)\n",
    "\n",
    "    if not eval_highway:\n",
    "        for i, pred in enumerate(preds_all):\n",
    "            if eval_task == 'rte' or 'qnli' or 'wnli' or 'qqp':\n",
    "                eval_task = 'scitail'\n",
    "            if eval_task == 'mnli':\n",
    "                eval_task = 'snli'\n",
    "            if eval_task == 'yelp':\n",
    "              eval_task = 'imdb'\n",
    "            result = elue_compute_metrics(eval_task, pred, out_label_ids)  # 실제 라벨과 비교해서 정확도/정밀도 등 평가 지표를 계산\n",
    "            results_all[i].update(result)\n",
    "\n",
    "    else:\n",
    "        if eval_task == 'rte' or 'qnli' or 'wnli' or 'qqp':\n",
    "                eval_task = 'scitail'\n",
    "        if eval_task == 'mnli':\n",
    "                eval_task = 'snli'\n",
    "        result = elue_compute_metrics(eval_task, preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            print(\"  %s = %s\" % (key, str(result[key])))\n",
    "\n",
    "        exiting_layer_every_ins = model.elasticbert.exiting_layer_every_ins\n",
    "        exit_layer.append(exiting_layer_every_ins)\n",
    "\n",
    "    if eval_highway:\n",
    "        speed_up = model.elasticbert.log_stats()\n",
    "        return results, speed_up, exit_layer\n",
    "\n",
    "    if args.spec_eval:\n",
    "      return results_all, preds_all, pred_tuple, out_label_ids\n",
    "\n",
    "    return results_all, preds_all, pred_tuple , out_label_ids\n",
    "    #return results_all, preds_all, out_label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jCJmL5-yez21"
   },
   "outputs": [],
   "source": [
    "ELUE_DIR='/mnt/aix7101/jeong/fnlp/elasticbert-base'\n",
    "TASK_NAME='IMDb'\n",
    "\n",
    "arg_vec= ['--model_name_or_path', 'fnlp/elasticbert-base',\n",
    "  '--task_name', 'IMDb', \\\n",
    "  '--do_train', \\\n",
    "  '--do_lower_case', \\\n",
    "  '--data_dir', \"/home/divya/UBERT/elue_data\", \\\n",
    "  '--log_dir', '/mnt/aix7101/jeong/fnlp/elasticbert-base/log/elue/entropy/SNLI-BTestCheck', \\\n",
    "  '--output_dir', '/mnt/aix7101/jeong/fnlp/elasticbert-base/ckpts/elue/entropy/SNLI-BTestCheck', \\\n",
    "  '--num_hidden_layers', '12', \\\n",
    "  '--num_output_layers', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '1', \\\n",
    "  '--max_seq_length', '128', \\\n",
    "  '--per_gpu_train_batch_size', '32', \\\n",
    "  '--per_gpu_eval_batch_size',' 32', \\\n",
    "  '--learning_rate', '2e-5', \\\n",
    "  '--weight_decay', '0.1', \\\n",
    "  '--save_steps', '50', \\\n",
    "  '--logging_steps', '50', \\\n",
    "  '--num_train_epochs', '5',  \\\n",
    "  '--warmup_rate', '0.06', \\\n",
    "  '--evaluate_during_training', \\\n",
    "  '--overwrite_output_dir'\n",
    "]\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = get_args(arg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLZpjqakoP9G",
    "outputId": "a9b77e92-8aaf-4f06-db5f-b87dd9aec42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "args.output_mode = 'classification'\n",
    "\n",
    "print(args.device)\n",
    "model.to(args.device)\n",
    "print(args.n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "50EjaE5MhPkU"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hsm207/imdb_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lfKacfaChWSJ"
   },
   "outputs": [],
   "source": [
    "# %cd imdb_data\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YsgrccgxiiQq"
   },
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 --infile create_imdb_dataset.py --outfile bar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vbLkqld8ipqI"
   },
   "outputs": [],
   "source": [
    "# !python bar.py --output_dir /home/divya/UBERT/elue_data/imdb_data/imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rNSuHnH-mSPt"
   },
   "outputs": [],
   "source": [
    "#Custom Selection\n",
    "# dataset = 'SNLI'#'IMDb' #or 'Yelp'\n",
    "# data_split = 'train'\n",
    "#To check model performance on SST-2 dev split:\n",
    "#Please set dataset = 'SST-2' and data_split='dev'\n",
    "dataset = 'IMDb'\n",
    "data_split='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZyrFjs52hwRW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_preds(eval_dataset='SNLI', data_split='train'):\n",
    "  args.spec_eval = False\n",
    "  args.task_name = eval_dataset.lower()\n",
    "  args.data_dir=ELUE_DIR + '/'+args.task_name\n",
    "\n",
    "  results_all, exit_preds, pred_tuple, op_labels = evaluate_elue_entropy(args, model, tokenizer)\n",
    "\n",
    "\n",
    "  # exit_preds_list = np.stack(exit_preds, axis=1)\n",
    "  # df = pd.DataFrame((exit_preds_list) )\n",
    "  # df['op_labels'] = op_labels\n",
    "\n",
    "  return  results_all, exit_preds, pred_tuple, op_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # 1. SNLI 데이터셋 불러오기\n",
    "# dataset = load_dataset(\"snli\", split=\"train\")\n",
    "\n",
    "# # 2. 사용할 컬럼만 선택 (라벨이 유효한 것만 필터링)\n",
    "# filtered = dataset.filter(lambda x: x['label'] != -1)\n",
    "\n",
    "# # 3. 데이터프레임으로 변환\n",
    "# df = pd.DataFrame({\n",
    "#     \"premise\": filtered[\"premise\"],\n",
    "#     \"hypothesis\": filtered[\"hypothesis\"],\n",
    "#     \"label\": filtered[\"label\"]\n",
    "# })\n",
    "\n",
    "# # 4. 저장 경로 설정\n",
    "# save_dir = \"/mnt/aix7101/jeong/fnlp/elasticbert-base/snli\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# save_path = os.path.join(save_dir, \"train.tsv\")\n",
    "\n",
    "# # 5. TSV 파일로 저장\n",
    "# df.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# print(f\"✅ Saved SNLI train.tsv to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 모델을 GPU에 올린 적이 있으면 해당 데이터에서 에러가 날 수 있으므로 아예 restart하는 것도 좋은 방법임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxJkyv_xHy-H",
    "outputId": "e676791a-51ed-49db-a84a-6e2a00c290e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/jeong/CeeBERT/ElasticBERT/finetune-dynamic/load_data.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(cached_features_file)\n",
      "Evaluating: 100%|██████████| 782/782 [00:31<00:00, 25.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from elue import elue_compute_metrics\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "results, final_preds, pred_tuple, op_labels = get_preds(eval_dataset=dataset, data_split=data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(pred_tuple[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "7\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(op_labels))\n",
    "print(len(final_preds))\n",
    "print(len(final_preds[0]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FnF4zrhPMYNV"
   },
   "outputs": [],
   "source": [
    "accurac_imd = []\n",
    "for j in range(sum(args.num_output_layers)):\n",
    "    accuracy = 0\n",
    "    for i in range(len(op_labels)):\n",
    "        if final_preds[j][i] == op_labels[i]:\n",
    "            accuracy+=1\n",
    "        else:\n",
    "            pass\n",
    "    accurac_imd.append(accuracy/len(op_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.49992, 0.63572, 0.7302, 0.73108, 0.78084, 0.81032]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurac_imd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accurac_yelp = []\n",
    "# for j in range(sum(args.num_output_layers)):\n",
    "#     accuracy = 0\n",
    "#     for i in range(len(op_labels)):\n",
    "#         if final_preds[j][i] == op_labels[i]:\n",
    "#             accuracy+=1\n",
    "#         else:\n",
    "#             pass\n",
    "#     accurac_yelp.append(accuracy/len(op_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1, 2, 3, 4, 5, 6, 7]\n",
    "plt.plot(x, accurac_imd, linestyle='dotted', marker = '^', color=\"r\")\n",
    "# plt.plot(x, accurac_yelp, linestyle='dotted', marker = 'o', color=\"g\")\n",
    "plt.xlabel(\"Layer number\", fontsize = 17)\n",
    "plt.ylabel(\"Accuracy\", fontsize = 17)\n",
    "plt.title(\"Accuracy vs Layer\", fontsize = 17)\n",
    "plt.legend(['IMDb'],\n",
    "        prop = {'size' : 15},\n",
    "        loc = 'lower right', shadow = True,\n",
    "        facecolor = 'white')\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.savefig(\"/home/aix7101/jeong/CeeBERT/ElasticBERT/Accuracy_vs_layer_Imdb and yelp.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dqNuO_askX0X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_thi = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_thi.append(max(softmax(pred_tuple[2][i])))\n",
    "# pred_proba_thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX8DlkDGWdQ-",
    "outputId": "639a3136-cd41-4455-e370-1d02f7e01204"
   },
   "outputs": [],
   "source": [
    "pred_prob_six = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_six.append(max(softmax(pred_tuple[5][i])))\n",
    "# pred_proba_fou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float32(0.5490912),\n",
       " np.float32(0.8932002),\n",
       " np.float32(0.8223526),\n",
       " np.float32(0.7375171),\n",
       " np.float32(0.8437855),\n",
       " np.float32(0.9293963),\n",
       " np.float32(0.8543703),\n",
       " np.float32(0.7049185),\n",
       " np.float32(0.88025826),\n",
       " np.float32(0.66299534),\n",
       " np.float32(0.7298023),\n",
       " np.float32(0.79754454),\n",
       " np.float32(0.9266994),\n",
       " np.float32(0.876034),\n",
       " np.float32(0.5074671),\n",
       " np.float32(0.9137964),\n",
       " np.float32(0.9015699),\n",
       " np.float32(0.9002454),\n",
       " np.float32(0.8773809),\n",
       " np.float32(0.84488887),\n",
       " np.float32(0.6867731),\n",
       " np.float32(0.8169792),\n",
       " np.float32(0.833848),\n",
       " np.float32(0.79851764),\n",
       " np.float32(0.87293166),\n",
       " np.float32(0.73482823),\n",
       " np.float32(0.8868725),\n",
       " np.float32(0.5308803),\n",
       " np.float32(0.8013094),\n",
       " np.float32(0.74192166),\n",
       " np.float32(0.920821),\n",
       " np.float32(0.8513628),\n",
       " np.float32(0.56488514),\n",
       " np.float32(0.79011965),\n",
       " np.float32(0.888481),\n",
       " np.float32(0.7829671),\n",
       " np.float32(0.89185804),\n",
       " np.float32(0.84108067),\n",
       " np.float32(0.9183559),\n",
       " np.float32(0.69913197),\n",
       " np.float32(0.8325104),\n",
       " np.float32(0.8561859),\n",
       " np.float32(0.75107414),\n",
       " np.float32(0.7393821),\n",
       " np.float32(0.84574527),\n",
       " np.float32(0.9254207),\n",
       " np.float32(0.89028454),\n",
       " np.float32(0.67724323),\n",
       " np.float32(0.8797234),\n",
       " np.float32(0.9086816),\n",
       " np.float32(0.58556795),\n",
       " np.float32(0.8710238),\n",
       " np.float32(0.8161647),\n",
       " np.float32(0.6166339),\n",
       " np.float32(0.8742825),\n",
       " np.float32(0.7852533),\n",
       " np.float32(0.93549925),\n",
       " np.float32(0.85423124),\n",
       " np.float32(0.8536687),\n",
       " np.float32(0.8748292),\n",
       " np.float32(0.6507375),\n",
       " np.float32(0.6039651),\n",
       " np.float32(0.9190535),\n",
       " np.float32(0.91090053),\n",
       " np.float32(0.86011755),\n",
       " np.float32(0.66936123),\n",
       " np.float32(0.8842769),\n",
       " np.float32(0.8866417),\n",
       " np.float32(0.744573),\n",
       " np.float32(0.5669196),\n",
       " np.float32(0.73720616),\n",
       " np.float32(0.72794414),\n",
       " np.float32(0.8722244),\n",
       " np.float32(0.86619556),\n",
       " np.float32(0.6353996),\n",
       " np.float32(0.9053255),\n",
       " np.float32(0.8572011),\n",
       " np.float32(0.7350256),\n",
       " np.float32(0.8585274),\n",
       " np.float32(0.6373536),\n",
       " np.float32(0.5539607),\n",
       " np.float32(0.7014656),\n",
       " np.float32(0.785613),\n",
       " np.float32(0.8961339),\n",
       " np.float32(0.840153),\n",
       " np.float32(0.7702053),\n",
       " np.float32(0.8285842),\n",
       " np.float32(0.749736),\n",
       " np.float32(0.87665856),\n",
       " np.float32(0.7578607),\n",
       " np.float32(0.692619),\n",
       " np.float32(0.714297),\n",
       " np.float32(0.62893),\n",
       " np.float32(0.8867455),\n",
       " np.float32(0.9334245),\n",
       " np.float32(0.66923934),\n",
       " np.float32(0.83417773),\n",
       " np.float32(0.83272433),\n",
       " np.float32(0.8544048),\n",
       " np.float32(0.9101285),\n",
       " np.float32(0.8888233),\n",
       " np.float32(0.90950733),\n",
       " np.float32(0.8547464),\n",
       " np.float32(0.84818256),\n",
       " np.float32(0.850515),\n",
       " np.float32(0.84543014),\n",
       " np.float32(0.9186543),\n",
       " np.float32(0.85511094),\n",
       " np.float32(0.5654655),\n",
       " np.float32(0.74900854),\n",
       " np.float32(0.88903046),\n",
       " np.float32(0.6635469),\n",
       " np.float32(0.85415304),\n",
       " np.float32(0.5238988),\n",
       " np.float32(0.8930344),\n",
       " np.float32(0.9139104),\n",
       " np.float32(0.81285733),\n",
       " np.float32(0.8194591),\n",
       " np.float32(0.9386388),\n",
       " np.float32(0.85156494),\n",
       " np.float32(0.87631303),\n",
       " np.float32(0.82943237),\n",
       " np.float32(0.896418),\n",
       " np.float32(0.60539955),\n",
       " np.float32(0.75498),\n",
       " np.float32(0.5965123),\n",
       " np.float32(0.717862),\n",
       " np.float32(0.6153324),\n",
       " np.float32(0.51521873),\n",
       " np.float32(0.86985683),\n",
       " np.float32(0.87960434),\n",
       " np.float32(0.88450694),\n",
       " np.float32(0.60624474),\n",
       " np.float32(0.8660283),\n",
       " np.float32(0.7218888),\n",
       " np.float32(0.8829718),\n",
       " np.float32(0.85355866),\n",
       " np.float32(0.71641207),\n",
       " np.float32(0.5121456),\n",
       " np.float32(0.7882477),\n",
       " np.float32(0.5549891),\n",
       " np.float32(0.8768583),\n",
       " np.float32(0.6688863),\n",
       " np.float32(0.84704006),\n",
       " np.float32(0.8577354),\n",
       " np.float32(0.5101077),\n",
       " np.float32(0.6654727),\n",
       " np.float32(0.76125157),\n",
       " np.float32(0.7810771),\n",
       " np.float32(0.7942854),\n",
       " np.float32(0.89405257),\n",
       " np.float32(0.90938723),\n",
       " np.float32(0.94050056),\n",
       " np.float32(0.9265624),\n",
       " np.float32(0.67989373),\n",
       " np.float32(0.84552467),\n",
       " np.float32(0.8670189),\n",
       " np.float32(0.7161003),\n",
       " np.float32(0.86089593),\n",
       " np.float32(0.8636133),\n",
       " np.float32(0.9017646),\n",
       " np.float32(0.91851526),\n",
       " np.float32(0.58776855),\n",
       " np.float32(0.8415799),\n",
       " np.float32(0.8490892),\n",
       " np.float32(0.7435633),\n",
       " np.float32(0.8939069),\n",
       " np.float32(0.500745),\n",
       " np.float32(0.500745),\n",
       " np.float32(0.768804),\n",
       " np.float32(0.8103452),\n",
       " np.float32(0.8010535),\n",
       " np.float32(0.770446),\n",
       " np.float32(0.7741456),\n",
       " np.float32(0.89459836),\n",
       " np.float32(0.9315746),\n",
       " np.float32(0.90844506),\n",
       " np.float32(0.8244485),\n",
       " np.float32(0.8638747),\n",
       " np.float32(0.7586197),\n",
       " np.float32(0.591442),\n",
       " np.float32(0.91082186),\n",
       " np.float32(0.6206061),\n",
       " np.float32(0.62379134),\n",
       " np.float32(0.93218917),\n",
       " np.float32(0.9237345),\n",
       " np.float32(0.92889583),\n",
       " np.float32(0.8214157),\n",
       " np.float32(0.745825),\n",
       " np.float32(0.87764037),\n",
       " np.float32(0.64215285),\n",
       " np.float32(0.8643796),\n",
       " np.float32(0.84508973),\n",
       " np.float32(0.8856182),\n",
       " np.float32(0.8517284),\n",
       " np.float32(0.8510329),\n",
       " np.float32(0.9279514),\n",
       " np.float32(0.80872923),\n",
       " np.float32(0.8143205),\n",
       " np.float32(0.6720351),\n",
       " np.float32(0.76982296),\n",
       " np.float32(0.92149043),\n",
       " np.float32(0.8724365),\n",
       " np.float32(0.62346625),\n",
       " np.float32(0.81573194),\n",
       " np.float32(0.82436347),\n",
       " np.float32(0.7600417),\n",
       " np.float32(0.8348249),\n",
       " np.float32(0.84887135),\n",
       " np.float32(0.8917663),\n",
       " np.float32(0.8437378),\n",
       " np.float32(0.8391573),\n",
       " np.float32(0.92279243),\n",
       " np.float32(0.6004646),\n",
       " np.float32(0.8987194),\n",
       " np.float32(0.89794534),\n",
       " np.float32(0.8818353),\n",
       " np.float32(0.8199462),\n",
       " np.float32(0.54536134),\n",
       " np.float32(0.66969985),\n",
       " np.float32(0.83653194),\n",
       " np.float32(0.8840089),\n",
       " np.float32(0.8588586),\n",
       " np.float32(0.7342763),\n",
       " np.float32(0.6781539),\n",
       " np.float32(0.73969924),\n",
       " np.float32(0.8437334),\n",
       " np.float32(0.9113343),\n",
       " np.float32(0.82257265),\n",
       " np.float32(0.88143057),\n",
       " np.float32(0.66355073),\n",
       " np.float32(0.8947794),\n",
       " np.float32(0.89982325),\n",
       " np.float32(0.89990646),\n",
       " np.float32(0.8676982),\n",
       " np.float32(0.79485095),\n",
       " np.float32(0.54978687),\n",
       " np.float32(0.70124954),\n",
       " np.float32(0.84337306),\n",
       " np.float32(0.8869472),\n",
       " np.float32(0.9046234),\n",
       " np.float32(0.7008528),\n",
       " np.float32(0.86295646),\n",
       " np.float32(0.9047156),\n",
       " np.float32(0.86578375),\n",
       " np.float32(0.66098624),\n",
       " np.float32(0.73407173),\n",
       " np.float32(0.8087012),\n",
       " np.float32(0.90229887),\n",
       " np.float32(0.8147565),\n",
       " np.float32(0.92422545),\n",
       " np.float32(0.83828133),\n",
       " np.float32(0.7191437),\n",
       " np.float32(0.86556894),\n",
       " np.float32(0.86241686),\n",
       " np.float32(0.7995273),\n",
       " np.float32(0.51537883),\n",
       " np.float32(0.79128075),\n",
       " np.float32(0.8037692),\n",
       " np.float32(0.76161754),\n",
       " np.float32(0.8124705),\n",
       " np.float32(0.82992023),\n",
       " np.float32(0.8494673),\n",
       " np.float32(0.5688879),\n",
       " np.float32(0.8078722),\n",
       " np.float32(0.8988852),\n",
       " np.float32(0.9124165),\n",
       " np.float32(0.7391046),\n",
       " np.float32(0.92886823),\n",
       " np.float32(0.8830497),\n",
       " np.float32(0.92082405),\n",
       " np.float32(0.62488633),\n",
       " np.float32(0.6988163),\n",
       " np.float32(0.9099391),\n",
       " np.float32(0.9077308),\n",
       " np.float32(0.8727009),\n",
       " np.float32(0.7886091),\n",
       " np.float32(0.7634617),\n",
       " np.float32(0.6901756),\n",
       " np.float32(0.74484175),\n",
       " np.float32(0.58373314),\n",
       " np.float32(0.5837091),\n",
       " np.float32(0.76530355),\n",
       " np.float32(0.6472278),\n",
       " np.float32(0.54536134),\n",
       " np.float32(0.5262814),\n",
       " np.float32(0.8711878),\n",
       " np.float32(0.78231156),\n",
       " np.float32(0.8652841),\n",
       " np.float32(0.64357436),\n",
       " np.float32(0.89662504),\n",
       " np.float32(0.89336836),\n",
       " np.float32(0.6121329),\n",
       " np.float32(0.72149515),\n",
       " np.float32(0.7636209),\n",
       " np.float32(0.80714405),\n",
       " np.float32(0.50260895),\n",
       " np.float32(0.90619755),\n",
       " np.float32(0.82752794),\n",
       " np.float32(0.7026183),\n",
       " np.float32(0.91760427),\n",
       " np.float32(0.57588446),\n",
       " np.float32(0.7551068),\n",
       " np.float32(0.84896904),\n",
       " np.float32(0.5881812),\n",
       " np.float32(0.91547096),\n",
       " np.float32(0.81210285),\n",
       " np.float32(0.7977025),\n",
       " np.float32(0.9131771),\n",
       " np.float32(0.90614456),\n",
       " np.float32(0.8422533),\n",
       " np.float32(0.8402328),\n",
       " np.float32(0.7806719),\n",
       " np.float32(0.75159633),\n",
       " np.float32(0.80017465),\n",
       " np.float32(0.92045367),\n",
       " np.float32(0.9167804),\n",
       " np.float32(0.84614354),\n",
       " np.float32(0.8454171),\n",
       " np.float32(0.86518675),\n",
       " np.float32(0.6994745),\n",
       " np.float32(0.8110643),\n",
       " np.float32(0.9227995),\n",
       " np.float32(0.8709023),\n",
       " np.float32(0.7395376),\n",
       " np.float32(0.8971444),\n",
       " np.float32(0.7699783),\n",
       " np.float32(0.7714681),\n",
       " np.float32(0.88588965),\n",
       " np.float32(0.72911686),\n",
       " np.float32(0.76152635),\n",
       " np.float32(0.9103081),\n",
       " np.float32(0.69926995),\n",
       " np.float32(0.8490299),\n",
       " np.float32(0.78396034),\n",
       " np.float32(0.73645926),\n",
       " np.float32(0.62994117),\n",
       " np.float32(0.7999639),\n",
       " np.float32(0.8546184),\n",
       " np.float32(0.8134716),\n",
       " np.float32(0.72059),\n",
       " np.float32(0.6222804),\n",
       " np.float32(0.7425169),\n",
       " np.float32(0.72303903),\n",
       " np.float32(0.90052253),\n",
       " np.float32(0.8727813),\n",
       " np.float32(0.7937063),\n",
       " np.float32(0.63596827),\n",
       " np.float32(0.63594675),\n",
       " np.float32(0.89007485),\n",
       " np.float32(0.8729168),\n",
       " np.float32(0.8120901),\n",
       " np.float32(0.7989206),\n",
       " np.float32(0.789972),\n",
       " np.float32(0.576502),\n",
       " np.float32(0.8155626),\n",
       " np.float32(0.9349622),\n",
       " np.float32(0.8946089),\n",
       " np.float32(0.8288933),\n",
       " np.float32(0.90727544),\n",
       " np.float32(0.7833435),\n",
       " np.float32(0.82813066),\n",
       " np.float32(0.5952304),\n",
       " np.float32(0.91533625),\n",
       " np.float32(0.6928374),\n",
       " np.float32(0.7860657),\n",
       " np.float32(0.79266393),\n",
       " np.float32(0.8781699),\n",
       " np.float32(0.9037811),\n",
       " np.float32(0.921152),\n",
       " np.float32(0.7907138),\n",
       " np.float32(0.68807113),\n",
       " np.float32(0.744593),\n",
       " np.float32(0.8407936),\n",
       " np.float32(0.86609036),\n",
       " np.float32(0.5151019),\n",
       " np.float32(0.6829458),\n",
       " np.float32(0.6107463),\n",
       " np.float32(0.83500457),\n",
       " np.float32(0.8397907),\n",
       " np.float32(0.9302546),\n",
       " np.float32(0.8391368),\n",
       " np.float32(0.9065926),\n",
       " np.float32(0.7376009),\n",
       " np.float32(0.57266974),\n",
       " np.float32(0.88933986),\n",
       " np.float32(0.84400177),\n",
       " np.float32(0.5897303),\n",
       " np.float32(0.8421499),\n",
       " np.float32(0.83972335),\n",
       " np.float32(0.91252726),\n",
       " np.float32(0.87785906),\n",
       " np.float32(0.73305243),\n",
       " np.float32(0.8360731),\n",
       " np.float32(0.5609111),\n",
       " np.float32(0.9084427),\n",
       " np.float32(0.868246),\n",
       " np.float32(0.6322188),\n",
       " np.float32(0.92596847),\n",
       " np.float32(0.9255237),\n",
       " np.float32(0.9371695),\n",
       " np.float32(0.76785415),\n",
       " np.float32(0.9231477),\n",
       " np.float32(0.54161197),\n",
       " np.float32(0.85106397),\n",
       " np.float32(0.5055002),\n",
       " np.float32(0.5256333),\n",
       " np.float32(0.9118633),\n",
       " np.float32(0.64001787),\n",
       " np.float32(0.93309444),\n",
       " np.float32(0.6022163),\n",
       " np.float32(0.88732326),\n",
       " np.float32(0.8683985),\n",
       " np.float32(0.5751239),\n",
       " np.float32(0.83251387),\n",
       " np.float32(0.7285071),\n",
       " np.float32(0.70401525),\n",
       " np.float32(0.8699217),\n",
       " np.float32(0.8586068),\n",
       " np.float32(0.7156278),\n",
       " np.float32(0.8612548),\n",
       " np.float32(0.6450526),\n",
       " np.float32(0.8885994),\n",
       " np.float32(0.7844257),\n",
       " np.float32(0.9069596),\n",
       " np.float32(0.8852964),\n",
       " np.float32(0.8721212),\n",
       " np.float32(0.7933741),\n",
       " np.float32(0.9328229),\n",
       " np.float32(0.86912835),\n",
       " np.float32(0.63074416),\n",
       " np.float32(0.8505863),\n",
       " np.float32(0.9388205),\n",
       " np.float32(0.78292054),\n",
       " np.float32(0.8329937),\n",
       " np.float32(0.9216566),\n",
       " np.float32(0.92026746),\n",
       " np.float32(0.9246322),\n",
       " np.float32(0.8074609),\n",
       " np.float32(0.9116785),\n",
       " np.float32(0.52278733),\n",
       " np.float32(0.54258645),\n",
       " np.float32(0.80154127),\n",
       " np.float32(0.56531596),\n",
       " np.float32(0.80136454),\n",
       " np.float32(0.90564674),\n",
       " np.float32(0.6533541),\n",
       " np.float32(0.8823736),\n",
       " np.float32(0.83489126),\n",
       " np.float32(0.9139461),\n",
       " np.float32(0.61329436),\n",
       " np.float32(0.900656),\n",
       " np.float32(0.8966979),\n",
       " np.float32(0.8992157),\n",
       " np.float32(0.917591),\n",
       " np.float32(0.5457349),\n",
       " np.float32(0.851792),\n",
       " np.float32(0.82457983),\n",
       " np.float32(0.6141393),\n",
       " np.float32(0.65866166),\n",
       " np.float32(0.88864654),\n",
       " np.float32(0.64673287),\n",
       " np.float32(0.9332565),\n",
       " np.float32(0.8167986),\n",
       " np.float32(0.53830075),\n",
       " np.float32(0.7818054),\n",
       " np.float32(0.6602465),\n",
       " np.float32(0.9240129),\n",
       " np.float32(0.65133953),\n",
       " np.float32(0.6980956),\n",
       " np.float32(0.8676475),\n",
       " np.float32(0.61143225),\n",
       " np.float32(0.6198422),\n",
       " np.float32(0.7498519),\n",
       " np.float32(0.7325182),\n",
       " np.float32(0.85862434),\n",
       " np.float32(0.6559465),\n",
       " np.float32(0.73370856),\n",
       " np.float32(0.8403253),\n",
       " np.float32(0.61025715),\n",
       " np.float32(0.91871387),\n",
       " np.float32(0.763235),\n",
       " np.float32(0.54227644),\n",
       " np.float32(0.8862015),\n",
       " np.float32(0.8680634),\n",
       " np.float32(0.9346413),\n",
       " np.float32(0.7110678),\n",
       " np.float32(0.88321596),\n",
       " np.float32(0.7232163),\n",
       " np.float32(0.8835751),\n",
       " np.float32(0.5353103),\n",
       " np.float32(0.74636847),\n",
       " np.float32(0.7042924),\n",
       " np.float32(0.8651653),\n",
       " np.float32(0.85817534),\n",
       " np.float32(0.8824136),\n",
       " np.float32(0.53355294),\n",
       " np.float32(0.5686746),\n",
       " np.float32(0.7514196),\n",
       " np.float32(0.6259052),\n",
       " np.float32(0.76784444),\n",
       " np.float32(0.66297495),\n",
       " np.float32(0.8677687),\n",
       " np.float32(0.91093814),\n",
       " np.float32(0.6870311),\n",
       " np.float32(0.8872627),\n",
       " np.float32(0.87471384),\n",
       " np.float32(0.82749856),\n",
       " np.float32(0.73039114),\n",
       " np.float32(0.5563357),\n",
       " np.float32(0.7343568),\n",
       " np.float32(0.8231146),\n",
       " np.float32(0.8760036),\n",
       " np.float32(0.6900767),\n",
       " np.float32(0.84617096),\n",
       " np.float32(0.52383775),\n",
       " np.float32(0.5148677),\n",
       " np.float32(0.85082537),\n",
       " np.float32(0.71666026),\n",
       " np.float32(0.6686495),\n",
       " np.float32(0.6166755),\n",
       " np.float32(0.8828683),\n",
       " np.float32(0.8018308),\n",
       " np.float32(0.8773863),\n",
       " np.float32(0.92804986),\n",
       " np.float32(0.8829992),\n",
       " np.float32(0.87918),\n",
       " np.float32(0.7085847),\n",
       " np.float32(0.74767065),\n",
       " np.float32(0.7996831),\n",
       " np.float32(0.7279133),\n",
       " np.float32(0.8825378),\n",
       " np.float32(0.7593594),\n",
       " np.float32(0.9171),\n",
       " np.float32(0.85865587),\n",
       " np.float32(0.87967336),\n",
       " np.float32(0.8875418),\n",
       " np.float32(0.8062621),\n",
       " np.float32(0.92682713),\n",
       " np.float32(0.92326576),\n",
       " np.float32(0.5757853),\n",
       " np.float32(0.73588115),\n",
       " np.float32(0.9094203),\n",
       " np.float32(0.53768593),\n",
       " np.float32(0.5003107),\n",
       " np.float32(0.8347187),\n",
       " np.float32(0.51663107),\n",
       " np.float32(0.8902186),\n",
       " np.float32(0.9035711),\n",
       " np.float32(0.9070558),\n",
       " np.float32(0.8778142),\n",
       " np.float32(0.81429064),\n",
       " np.float32(0.8546621),\n",
       " np.float32(0.7221568),\n",
       " np.float32(0.86694324),\n",
       " np.float32(0.8918628),\n",
       " np.float32(0.8665785),\n",
       " np.float32(0.83182734),\n",
       " np.float32(0.9431419),\n",
       " np.float32(0.7510269),\n",
       " np.float32(0.7381821),\n",
       " np.float32(0.9101893),\n",
       " np.float32(0.8791425),\n",
       " np.float32(0.55720526),\n",
       " np.float32(0.5865153),\n",
       " np.float32(0.86765844),\n",
       " np.float32(0.80554426),\n",
       " np.float32(0.7548604),\n",
       " np.float32(0.85235065),\n",
       " np.float32(0.50319767),\n",
       " np.float32(0.8085478),\n",
       " np.float32(0.9149201),\n",
       " np.float32(0.70809937),\n",
       " np.float32(0.9084849),\n",
       " np.float32(0.84243727),\n",
       " np.float32(0.8811093),\n",
       " np.float32(0.91083455),\n",
       " np.float32(0.7307256),\n",
       " np.float32(0.79891485),\n",
       " np.float32(0.8510437),\n",
       " np.float32(0.9092177),\n",
       " np.float32(0.82753325),\n",
       " np.float32(0.88970256),\n",
       " np.float32(0.87905204),\n",
       " np.float32(0.88387567),\n",
       " np.float32(0.943772),\n",
       " np.float32(0.84501016),\n",
       " np.float32(0.8530281),\n",
       " np.float32(0.5055547),\n",
       " np.float32(0.8529399),\n",
       " np.float32(0.90549004),\n",
       " np.float32(0.64705485),\n",
       " np.float32(0.55038816),\n",
       " np.float32(0.59376365),\n",
       " np.float32(0.67256236),\n",
       " np.float32(0.71518666),\n",
       " np.float32(0.8406355),\n",
       " np.float32(0.88712883),\n",
       " np.float32(0.8343557),\n",
       " np.float32(0.8458537),\n",
       " np.float32(0.6376858),\n",
       " np.float32(0.8970119),\n",
       " np.float32(0.8031346),\n",
       " np.float32(0.55192894),\n",
       " np.float32(0.8360663),\n",
       " np.float32(0.7609039),\n",
       " np.float32(0.8279916),\n",
       " np.float32(0.71603495),\n",
       " np.float32(0.8695892),\n",
       " np.float32(0.8673189),\n",
       " np.float32(0.8117584),\n",
       " np.float32(0.8395808),\n",
       " np.float32(0.5931453),\n",
       " np.float32(0.91104513),\n",
       " np.float32(0.65608823),\n",
       " np.float32(0.5112262),\n",
       " np.float32(0.8828995),\n",
       " np.float32(0.9169435),\n",
       " np.float32(0.64995426),\n",
       " np.float32(0.8511205),\n",
       " np.float32(0.8844505),\n",
       " np.float32(0.7999929),\n",
       " np.float32(0.8695504),\n",
       " np.float32(0.5822931),\n",
       " np.float32(0.5379898),\n",
       " np.float32(0.65354514),\n",
       " np.float32(0.7165842),\n",
       " np.float32(0.79859346),\n",
       " np.float32(0.8283202),\n",
       " np.float32(0.69653773),\n",
       " np.float32(0.88584876),\n",
       " np.float32(0.85833734),\n",
       " np.float32(0.85084426),\n",
       " np.float32(0.85694903),\n",
       " np.float32(0.7790367),\n",
       " np.float32(0.8737632),\n",
       " np.float32(0.9146678),\n",
       " np.float32(0.93550664),\n",
       " np.float32(0.88020825),\n",
       " np.float32(0.8103761),\n",
       " np.float32(0.9174803),\n",
       " np.float32(0.907185),\n",
       " np.float32(0.7535148),\n",
       " np.float32(0.8783378),\n",
       " np.float32(0.8420795),\n",
       " np.float32(0.63815665),\n",
       " np.float32(0.9308375),\n",
       " np.float32(0.7947644),\n",
       " np.float32(0.906472),\n",
       " np.float32(0.7253827),\n",
       " np.float32(0.71320117),\n",
       " np.float32(0.6619673),\n",
       " np.float32(0.7986069),\n",
       " np.float32(0.8031411),\n",
       " np.float32(0.74322104),\n",
       " np.float32(0.54857045),\n",
       " np.float32(0.5549227),\n",
       " np.float32(0.8841826),\n",
       " np.float32(0.8401201),\n",
       " np.float32(0.8685048),\n",
       " np.float32(0.682129),\n",
       " np.float32(0.8252298),\n",
       " np.float32(0.88314825),\n",
       " np.float32(0.9310299),\n",
       " np.float32(0.9310299),\n",
       " np.float32(0.7954996),\n",
       " np.float32(0.8513942),\n",
       " np.float32(0.762835),\n",
       " np.float32(0.8930219),\n",
       " np.float32(0.72340935),\n",
       " np.float32(0.91263336),\n",
       " np.float32(0.5241338),\n",
       " np.float32(0.8374812),\n",
       " np.float32(0.9008719),\n",
       " np.float32(0.7081468),\n",
       " np.float32(0.7948545),\n",
       " np.float32(0.7031057),\n",
       " np.float32(0.8917233),\n",
       " np.float32(0.8765501),\n",
       " np.float32(0.8593808),\n",
       " np.float32(0.8255098),\n",
       " np.float32(0.8061377),\n",
       " np.float32(0.9215659),\n",
       " np.float32(0.6798887),\n",
       " np.float32(0.90577185),\n",
       " np.float32(0.62077594),\n",
       " np.float32(0.8434883),\n",
       " np.float32(0.73192436),\n",
       " np.float32(0.5040259),\n",
       " np.float32(0.8217583),\n",
       " np.float32(0.8069342),\n",
       " np.float32(0.9073597),\n",
       " np.float32(0.8870655),\n",
       " np.float32(0.8336461),\n",
       " np.float32(0.839433),\n",
       " np.float32(0.7362296),\n",
       " np.float32(0.88721013),\n",
       " np.float32(0.76524764),\n",
       " np.float32(0.87962663),\n",
       " np.float32(0.65483433),\n",
       " np.float32(0.91936827),\n",
       " np.float32(0.65483433),\n",
       " np.float32(0.91218966),\n",
       " np.float32(0.54643154),\n",
       " np.float32(0.7100776),\n",
       " np.float32(0.8564681),\n",
       " np.float32(0.7247445),\n",
       " np.float32(0.761971),\n",
       " np.float32(0.89400315),\n",
       " np.float32(0.77529824),\n",
       " np.float32(0.58912635),\n",
       " np.float32(0.7637765),\n",
       " np.float32(0.78168416),\n",
       " np.float32(0.8959858),\n",
       " np.float32(0.86711425),\n",
       " np.float32(0.7801828),\n",
       " np.float32(0.9409795),\n",
       " np.float32(0.82940435),\n",
       " np.float32(0.89502466),\n",
       " np.float32(0.607438),\n",
       " np.float32(0.7891161),\n",
       " np.float32(0.6938713),\n",
       " np.float32(0.8917108),\n",
       " np.float32(0.88942355),\n",
       " np.float32(0.6381069),\n",
       " np.float32(0.74878067),\n",
       " np.float32(0.8346996),\n",
       " np.float32(0.7086908),\n",
       " np.float32(0.64121497),\n",
       " np.float32(0.52203375),\n",
       " np.float32(0.7832721),\n",
       " np.float32(0.7415578),\n",
       " np.float32(0.84713656),\n",
       " np.float32(0.87828606),\n",
       " np.float32(0.8855221),\n",
       " np.float32(0.641155),\n",
       " np.float32(0.9449289),\n",
       " np.float32(0.8416046),\n",
       " np.float32(0.7007542),\n",
       " np.float32(0.7206105),\n",
       " np.float32(0.824632),\n",
       " np.float32(0.8947973),\n",
       " np.float32(0.6707501),\n",
       " np.float32(0.6336871),\n",
       " np.float32(0.8752183),\n",
       " np.float32(0.86148703),\n",
       " np.float32(0.83011776),\n",
       " np.float32(0.87667024),\n",
       " np.float32(0.8675008),\n",
       " np.float32(0.8582031),\n",
       " np.float32(0.5847865),\n",
       " np.float32(0.84564745),\n",
       " np.float32(0.8637657),\n",
       " np.float32(0.5311499),\n",
       " np.float32(0.81810194),\n",
       " np.float32(0.9390774),\n",
       " np.float32(0.84434795),\n",
       " np.float32(0.9300262),\n",
       " np.float32(0.716394),\n",
       " np.float32(0.85947394),\n",
       " np.float32(0.93483096),\n",
       " np.float32(0.8711417),\n",
       " np.float32(0.6741526),\n",
       " np.float32(0.87232167),\n",
       " np.float32(0.89183176),\n",
       " np.float32(0.80524725),\n",
       " np.float32(0.66862553),\n",
       " np.float32(0.5023954),\n",
       " np.float32(0.5477873),\n",
       " np.float32(0.88457996),\n",
       " np.float32(0.8805986),\n",
       " np.float32(0.88814926),\n",
       " np.float32(0.5771426),\n",
       " np.float32(0.8853644),\n",
       " np.float32(0.85201347),\n",
       " np.float32(0.862962),\n",
       " np.float32(0.619563),\n",
       " np.float32(0.69488853),\n",
       " np.float32(0.8261501),\n",
       " np.float32(0.6201783),\n",
       " np.float32(0.92105097),\n",
       " np.float32(0.8305086),\n",
       " np.float32(0.87645775),\n",
       " np.float32(0.65764177),\n",
       " np.float32(0.87205136),\n",
       " np.float32(0.8608322),\n",
       " np.float32(0.5224681),\n",
       " np.float32(0.8200428),\n",
       " np.float32(0.6964502),\n",
       " np.float32(0.7179513),\n",
       " np.float32(0.5974824),\n",
       " np.float32(0.85425586),\n",
       " np.float32(0.7060602),\n",
       " np.float32(0.7451811),\n",
       " np.float32(0.69288415),\n",
       " np.float32(0.9078392),\n",
       " np.float32(0.5003169),\n",
       " np.float32(0.80261904),\n",
       " np.float32(0.94472736),\n",
       " np.float32(0.65221274),\n",
       " np.float32(0.90282357),\n",
       " np.float32(0.93440396),\n",
       " np.float32(0.7871765),\n",
       " np.float32(0.7593165),\n",
       " np.float32(0.77024525),\n",
       " np.float32(0.9031173),\n",
       " np.float32(0.9202042),\n",
       " np.float32(0.9053559),\n",
       " np.float32(0.8714103),\n",
       " np.float32(0.83524),\n",
       " np.float32(0.77026665),\n",
       " np.float32(0.8691177),\n",
       " np.float32(0.80948466),\n",
       " np.float32(0.8558411),\n",
       " np.float32(0.6659399),\n",
       " np.float32(0.8058818),\n",
       " np.float32(0.87341505),\n",
       " np.float32(0.794403),\n",
       " np.float32(0.824357),\n",
       " np.float32(0.9386777),\n",
       " np.float32(0.79107004),\n",
       " np.float32(0.8383772),\n",
       " np.float32(0.7968456),\n",
       " np.float32(0.8610535),\n",
       " np.float32(0.7073168),\n",
       " np.float32(0.799625),\n",
       " np.float32(0.6790609),\n",
       " np.float32(0.59830356),\n",
       " np.float32(0.6387607),\n",
       " np.float32(0.558674),\n",
       " np.float32(0.8167127),\n",
       " np.float32(0.7954917),\n",
       " np.float32(0.8009204),\n",
       " np.float32(0.8532334),\n",
       " np.float32(0.74902874),\n",
       " np.float32(0.84742683),\n",
       " np.float32(0.63247126),\n",
       " np.float32(0.7450285),\n",
       " np.float32(0.90195143),\n",
       " np.float32(0.8915345),\n",
       " np.float32(0.91821176),\n",
       " np.float32(0.9006999),\n",
       " np.float32(0.6961048),\n",
       " np.float32(0.8811758),\n",
       " np.float32(0.66894376),\n",
       " np.float32(0.5333988),\n",
       " np.float32(0.8280847),\n",
       " np.float32(0.7952588),\n",
       " np.float32(0.80733216),\n",
       " np.float32(0.8635928),\n",
       " np.float32(0.91651344),\n",
       " np.float32(0.87955445),\n",
       " np.float32(0.5745973),\n",
       " np.float32(0.84627277),\n",
       " np.float32(0.8683721),\n",
       " np.float32(0.8792289),\n",
       " np.float32(0.93714684),\n",
       " np.float32(0.83806866),\n",
       " np.float32(0.85834855),\n",
       " np.float32(0.66619116),\n",
       " np.float32(0.65273243),\n",
       " np.float32(0.8230694),\n",
       " np.float32(0.67140007),\n",
       " np.float32(0.652042),\n",
       " np.float32(0.8813305),\n",
       " np.float32(0.8505271),\n",
       " np.float32(0.8690491),\n",
       " np.float32(0.67797744),\n",
       " np.float32(0.7075141),\n",
       " np.float32(0.8714684),\n",
       " np.float32(0.84432155),\n",
       " np.float32(0.52016544),\n",
       " np.float32(0.66971606),\n",
       " np.float32(0.8929428),\n",
       " np.float32(0.8701537),\n",
       " np.float32(0.920712),\n",
       " np.float32(0.5289596),\n",
       " np.float32(0.7640341),\n",
       " np.float32(0.91942644),\n",
       " np.float32(0.5306922),\n",
       " np.float32(0.91802126),\n",
       " np.float32(0.81740093),\n",
       " np.float32(0.8836837),\n",
       " np.float32(0.9300641),\n",
       " np.float32(0.5361227),\n",
       " np.float32(0.7223353),\n",
       " np.float32(0.90593517),\n",
       " np.float32(0.88059396),\n",
       " np.float32(0.7808985),\n",
       " np.float32(0.9235896),\n",
       " np.float32(0.88891375),\n",
       " np.float32(0.6427282),\n",
       " np.float32(0.91087526),\n",
       " np.float32(0.5278899),\n",
       " np.float32(0.8847615),\n",
       " np.float32(0.77858365),\n",
       " np.float32(0.8155142),\n",
       " np.float32(0.9005558),\n",
       " np.float32(0.5270899),\n",
       " np.float32(0.8303676),\n",
       " np.float32(0.79711634),\n",
       " np.float32(0.8419498),\n",
       " np.float32(0.84361655),\n",
       " np.float32(0.82707804),\n",
       " np.float32(0.9106208),\n",
       " np.float32(0.79545856),\n",
       " np.float32(0.6595369),\n",
       " np.float32(0.76912993),\n",
       " np.float32(0.7745918),\n",
       " np.float32(0.77165115),\n",
       " np.float32(0.68363845),\n",
       " np.float32(0.83115786),\n",
       " np.float32(0.91803765),\n",
       " np.float32(0.5733797),\n",
       " np.float32(0.6558911),\n",
       " np.float32(0.65413886),\n",
       " np.float32(0.6187189),\n",
       " np.float32(0.9252753),\n",
       " np.float32(0.8014223),\n",
       " np.float32(0.73732823),\n",
       " np.float32(0.7779106),\n",
       " np.float32(0.9328032),\n",
       " np.float32(0.7954412),\n",
       " np.float32(0.8895349),\n",
       " np.float32(0.77069795),\n",
       " np.float32(0.8830299),\n",
       " np.float32(0.6625738),\n",
       " np.float32(0.8900041),\n",
       " np.float32(0.82982755),\n",
       " np.float32(0.86261606),\n",
       " np.float32(0.71513146),\n",
       " np.float32(0.93784183),\n",
       " np.float32(0.67217374),\n",
       " np.float32(0.7885142),\n",
       " np.float32(0.88048023),\n",
       " np.float32(0.9342819),\n",
       " np.float32(0.7304904),\n",
       " np.float32(0.7653247),\n",
       " np.float32(0.8100545),\n",
       " np.float32(0.67053837),\n",
       " np.float32(0.6000992),\n",
       " np.float32(0.7395696),\n",
       " np.float32(0.8367902),\n",
       " np.float32(0.87696826),\n",
       " np.float32(0.62898505),\n",
       " np.float32(0.8438845),\n",
       " np.float32(0.6813477),\n",
       " np.float32(0.8937875),\n",
       " np.float32(0.505775),\n",
       " np.float32(0.7416498),\n",
       " np.float32(0.7342302),\n",
       " np.float32(0.6820056),\n",
       " np.float32(0.73880243),\n",
       " np.float32(0.92972827),\n",
       " np.float32(0.65188146),\n",
       " np.float32(0.6061855),\n",
       " np.float32(0.7104793),\n",
       " np.float32(0.85491514),\n",
       " np.float32(0.82476175),\n",
       " np.float32(0.74843234),\n",
       " np.float32(0.81135947),\n",
       " np.float32(0.8514238),\n",
       " np.float32(0.86227417),\n",
       " np.float32(0.74897254),\n",
       " np.float32(0.5393508),\n",
       " np.float32(0.83912086),\n",
       " np.float32(0.8268524),\n",
       " np.float32(0.7749536),\n",
       " np.float32(0.81096447),\n",
       " np.float32(0.5886671),\n",
       " np.float32(0.9179051),\n",
       " np.float32(0.76252747),\n",
       " np.float32(0.84938365),\n",
       " np.float32(0.89054686),\n",
       " np.float32(0.8050155),\n",
       " np.float32(0.6684794),\n",
       " np.float32(0.7037849),\n",
       " np.float32(0.87756073),\n",
       " np.float32(0.9089704),\n",
       " np.float32(0.8470979),\n",
       " np.float32(0.89578664),\n",
       " np.float32(0.83512545),\n",
       " np.float32(0.8833604),\n",
       " np.float32(0.8531713),\n",
       " np.float32(0.82999265),\n",
       " np.float32(0.75921226),\n",
       " np.float32(0.9225855),\n",
       " np.float32(0.8219058),\n",
       " np.float32(0.90173525),\n",
       " np.float32(0.8681587),\n",
       " np.float32(0.6750436),\n",
       " np.float32(0.9136587),\n",
       " np.float32(0.91493773),\n",
       " np.float32(0.68071675),\n",
       " np.float32(0.7452288),\n",
       " np.float32(0.8823708),\n",
       " np.float32(0.8731438),\n",
       " np.float32(0.89293665),\n",
       " np.float32(0.8878862),\n",
       " np.float32(0.7777546),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_las = []\n",
    "for i in range(len(pred_tuple[0])):\n",
    "    pred_prob_las.append(max(softmax(pred_tuple[-1][i])))\n",
    "pred_prob_las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.69726133)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pred_prob_thi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_labels_1 = []\n",
    "for i in op_labels:\n",
    "    if i == 0:\n",
    "        op_labels_1.append(1)\n",
    "    elif i==1:\n",
    "        op_labels_1.append(2)\n",
    "    else:\n",
    "        op_labels_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gGJUAlUIH3SP"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(final_preds[2],final_preds[5], final_preds[-1], pred_prob_thi, pred_prob_six, pred_prob_las, op_labels_1)), columns =['Thi_layer_P','Six_layer_P', 'Last_layer','PProb_thi', 'PProb_six', 'PProb_las', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "H8RVPJMeXU9E"
   },
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], final_preds[7], final_preds[8], final_preds[9], final_preds[10], final_preds[11], op_labels)), columns =['Fir_p', 'Sec_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Six_p', 'Sev_p', 'Eig_p', 'Nin_p', 'Ten_p', 'Ele_p', 'Twe_p', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SJdbOqbi2AiL"
   },
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame(list(zip(final_preds[0], final_preds[1], final_preds[2], final_preds[3], final_preds[4], final_preds[5], final_preds[6], op_labels)), columns =['Fir_p', 'Thi_p', 'Fou_p', 'Fiv_p', 'Sev_p', 'Nin_p', 'Twe_p', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "eePvXaR2g44c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thi_layer_P</th>\n",
       "      <th>Six_layer_P</th>\n",
       "      <th>Last_layer</th>\n",
       "      <th>PProb_thi</th>\n",
       "      <th>PProb_six</th>\n",
       "      <th>PProb_las</th>\n",
       "      <th>True_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519159</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.549091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.522993</td>\n",
       "      <td>0.832413</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.548643</td>\n",
       "      <td>0.675569</td>\n",
       "      <td>0.822353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550421</td>\n",
       "      <td>0.778026</td>\n",
       "      <td>0.737517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.535718</td>\n",
       "      <td>0.776057</td>\n",
       "      <td>0.843786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thi_layer_P  Six_layer_P  Last_layer  PProb_thi  PProb_six  PProb_las  \\\n",
       "0            0            1           0   0.519159   0.524096   0.549091   \n",
       "1            0            0           0   0.522993   0.832413   0.893200   \n",
       "2            1            0           0   0.548643   0.675569   0.822353   \n",
       "3            1            0           0   0.550421   0.778026   0.737517   \n",
       "4            0            0           0   0.535718   0.776057   0.843786   \n",
       "\n",
       "   True_labels  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gfj1nbWqfea4",
    "outputId": "967cc785-e147-4b62-a3e5-29c02a8df442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.65212\n",
      "Accuracy =  0.39564\n",
      "Accuracy =  0.39752\n",
      "Accuracy =  0.0\n",
      "Accuracy =  0.0\n",
      "Accuracy =  0.0\n",
      "Accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_imdb = []\n",
    "for j in df.columns:\n",
    "  accuracy = 0\n",
    "  for i in range(df.shape[0]):\n",
    "      if df[j][i] == df['True_labels'][i]:\n",
    "          accuracy += 1\n",
    "      else:\n",
    "          pass\n",
    "  print(\"Accuracy = \", accuracy/df.shape[0])\n",
    "  accuracy_imdb.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "piV1ZUbg1zhc",
    "outputId": "70c2daee-416d-4598-8ff0-8aaf349d899f"
   },
   "outputs": [],
   "source": [
    "# accuracy_yelp = []\n",
    "# for j in df2.columns:\n",
    "#   accuracy = 0\n",
    "#   for i in range(df1.shape[0]):\n",
    "#       if df2[j][i] == df2['True_labels'][i]:\n",
    "#           accuracy += 1\n",
    "#       else:\n",
    "#           pass\n",
    "#   print(\"Accuracy = \", accuracy/df2.shape[0])\n",
    "#   accuracy_imdb.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "mgX5cPgt8vZr"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/aix7101/jeong/CeeBERT/ElasticBERT/Early_Exit_Confidence_data_snli_max_exits(3,6,12)_difference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "dE2200MMhJz1"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(r\"/UBERT/CSV_files/Early_Exit_Confidence_data_SST2_new_exits(12,12)_difference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tup_df(tuple):\n",
    "    pred_prob_thi = []\n",
    "    for i in range(len(tuple[2][0])):\n",
    "        pred_prob_thi.append(max(softmax(tuple[2][2][i])))\n",
    "\n",
    "\n",
    "    pred_prob_six = []\n",
    "    for i in range(len(tuple[2][0])):\n",
    "        pred_prob_six.append(max(softmax(tuple[2][5][i])))\n",
    "\n",
    "\n",
    "    pred_prob_las = []\n",
    "    for i in range(len(tuple[2][0])):\n",
    "        pred_prob_las.append(max(softmax(tuple[2][-1][i])))\n",
    "\n",
    "    op_labels_1 = []\n",
    "    for i in tuple[3]:\n",
    "        if i == 0:\n",
    "            op_labels_1.append(1)\n",
    "        elif i==1:\n",
    "            op_labels_1.append(2)\n",
    "        else:\n",
    "            op_labels_1.append(0)\n",
    "    return pred_prob_thi, pred_prob_six, pred_prob_las, op_labels_1\n",
    "\n",
    "df = pd.DataFrame(list(zip(final_preds[2],final_preds[5], final_preds[-1], pred_prob_thi, pred_prob_six, pred_prob_las, op_labels_1)), columns =['Thi_layer_P','Six_layer_P', 'Last_layer','PProb_thi', 'PProb_six', 'PProb_las', 'True_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thi_layer_P</th>\n",
       "      <th>Six_layer_P</th>\n",
       "      <th>Last_layer</th>\n",
       "      <th>PProb_thi</th>\n",
       "      <th>PProb_six</th>\n",
       "      <th>PProb_las</th>\n",
       "      <th>True_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519159</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.549091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.522993</td>\n",
       "      <td>0.832413</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.548643</td>\n",
       "      <td>0.675569</td>\n",
       "      <td>0.822353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550421</td>\n",
       "      <td>0.778026</td>\n",
       "      <td>0.737517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.535718</td>\n",
       "      <td>0.776057</td>\n",
       "      <td>0.843786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.552878</td>\n",
       "      <td>0.543862</td>\n",
       "      <td>0.546161</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508136</td>\n",
       "      <td>0.529887</td>\n",
       "      <td>0.503065</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590671</td>\n",
       "      <td>0.816035</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521273</td>\n",
       "      <td>0.500836</td>\n",
       "      <td>0.685543</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511541</td>\n",
       "      <td>0.891194</td>\n",
       "      <td>0.922338</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thi_layer_P  Six_layer_P  Last_layer  PProb_thi  PProb_six  PProb_las  \\\n",
       "0                0            1           0   0.519159   0.524096   0.549091   \n",
       "1                0            0           0   0.522993   0.832413   0.893200   \n",
       "2                1            0           0   0.548643   0.675569   0.822353   \n",
       "3                1            0           0   0.550421   0.778026   0.737517   \n",
       "4                0            0           0   0.535718   0.776057   0.843786   \n",
       "...            ...          ...         ...        ...        ...        ...   \n",
       "24995            1            0           0   0.552878   0.543862   0.546161   \n",
       "24996            1            0           1   0.508136   0.529887   0.503065   \n",
       "24997            1            1           1   0.590671   0.816035   0.812379   \n",
       "24998            0            1           1   0.521273   0.500836   0.685543   \n",
       "24999            1            1           1   0.511541   0.891194   0.922338   \n",
       "\n",
       "       True_labels  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "24995            2  \n",
       "24996            2  \n",
       "24997            2  \n",
       "24998            2  \n",
       "24999            2  \n",
       "\n",
       "[25000 rows x 7 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOpSU7ShmMU6",
    "outputId": "34d6d257-a43b-4d4f-d2d3-9396cf5fa963"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/jeong/CeeBERT/ElasticBERT/finetune-dynamic/load_data.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(cached_features_file)\n",
      "Evaluating: 100%|██████████| 782/782 [00:30<00:00, 25.78it/s]\n",
      "Evaluating: 100%|██████████| 782/782 [00:31<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Thi_layer_P  Six_layer_P  Last_layer  PProb_thi  PProb_six  PProb_las  \\\n",
      "0            0            1           0   0.519159   0.524096   0.549091   \n",
      "1            0            0           0   0.522993   0.832413   0.893200   \n",
      "2            1            0           0   0.548643   0.675569   0.822353   \n",
      "3            1            0           0   0.550421   0.778026   0.737517   \n",
      "4            0            0           0   0.535718   0.776057   0.843786   \n",
      "\n",
      "   True_labels  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/aix7101/jeong/CeeBERT/Early_Exits_Divya/Model_exit_predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m df_tot \u001b[38;5;241m=\u001b[39m df_tot\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_tot\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdf_tot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/aix7101/jeong/CeeBERT/Early_Exits_Divya/Model_exit_predictions/Exit_Predictions_TrainTest_IMDb_8exits.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_ceebert/lib/python3.9/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/aix7101/jeong/CeeBERT/Early_Exits_Divya/Model_exit_predictions'"
     ]
    }
   ],
   "source": [
    "df_train = get_preds(eval_dataset=dataset, data_split='train')\n",
    "df_test = get_preds(eval_dataset=dataset, data_split='test')\n",
    "\n",
    "\n",
    "a, b, c, d = tup_df(df_train)\n",
    "df_train = pd.DataFrame(list(zip(df_train[1][2],df_train[1][5], df_train[1][-1], a, b, c, d)), columns =['Thi_layer_P','Six_layer_P', 'Last_layer','PProb_thi', 'PProb_six', 'PProb_las', 'True_labels'])\n",
    "a, b, c, d = tup_df(df_test)\n",
    "df_test = pd.DataFrame(list(zip(df_test[1][2],df_test[1][5], df_test[1][-1], a, b, c, d)), columns =['Thi_layer_P','Six_layer_P', 'Last_layer','PProb_thi', 'PProb_six', 'PProb_las', 'True_labels'])\n",
    "\n",
    "df_tot = pd.concat([df_train, df_test])\n",
    "df_tot = df_tot.reset_index(drop=True)\n",
    "print(df_tot.head())\n",
    "\n",
    "df_tot.to_csv(r'/home/aix7101/jeong/CeeBERT/ElasticBERT/Exit_Predictions_TrainTest_IMDb_8exits.csv',sep ='\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "j_ceebert",
   "language": "python",
   "name": "j_ceebert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
